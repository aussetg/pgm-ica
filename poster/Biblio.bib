Automatically generated by Mendeley Desktop 1.17.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Shen2009,
author = {Shen, Hao and Jegelka, Stefanie and Gretton, Arthur},
number = {X},
title = {{Fast Kernel-Based Independent Component Analysis}},
volume = {X},
year = {2009}
}
@article{Lawrence2000,
author = {Lawrence, Neil D and Bishop, Christopher M},
file = {:Users/guillaume/Documents/Mendeley Desktop/Lawrence, Bishop - 2000 - Variational Bayesian independent component analysis.pdf:pdf},
journal = {Univ of Cambridge Tech Report},
number = {2},
title = {{Variational Bayesian independent component analysis}},
year = {2000}
}
@phdthesis{Choudrey2002,
author = {Choudrey, Rizwan A},
file = {:Users/guillaume/Documents/Mendeley Desktop/Choudrey - 2002 - Variational Methods for Bayesian Independent Component Analysis.pdf:pdf},
title = {{Variational Methods for Bayesian Independent Component Analysis}},
year = {2002}
}
@article{Bach2002,
author = {Bach, Francis R and Jordan, Michael I},
doi = {10.1162/153244303768966085},
file = {:Users/guillaume/Documents/Mendeley Desktop/Bach, Jordan - 2002 - Kernel Independent Component Analysis.pdf:pdf},
isbn = {0780376633},
issn = {0003-6951},
journal = {Journal of Machine Learning Research},
keywords = {blind source separation,canonical correlations,gral equations,gram matrices,incomplete cholesky decomposition,independent component analysis,inte-,kernel methods,mutual information,semiparametric models,stiefel manifold},
pages = {1--48},
pmid = {181462700001},
title = {{Kernel Independent Component Analysis}},
volume = {3},
year = {2002}
}
@article{Cardoso2003,
abstract = {Abstract Independent component analysis (ICA) is the decomposition of a random vector in linear components which are" as independent as possible." Here," independence" should be understood in its strong statistical sense: it goes beyond (second-order) decorrelation ... $\backslash$n},
author = {Cardoso, Jean-Fran{\c{c}}ois},
doi = {10.1162/jmlr.2003.4.7-8.1177},
file = {:Users/guillaume/Documents/Mendeley Desktop/Cardoso - 2003 - Dependence, correlation and Gaussianity in independent component analysis.pdf:pdf},
isbn = {1532-4435},
issn = {0003-6951},
journal = {The Journal of Machine Learning Research},
keywords = {cumulant expansions,independent component analysis,information,information geometry,minimum entropy,mutual,non-gaussianity,source separation},
pages = {1177--1203},
pmid = {501},
title = {{Dependence, correlation and Gaussianity in independent component analysis}},
url = {http://dl.acm.org/citation.cfm?id=945365.964302{\%}5Cnpapers2://publication/uuid/D8E63C91-C250-49FC-92A0-E13348D8F125},
volume = {4},
year = {2003}
}
@article{Shen2007,
abstract = {Recent approaches to independent component analysis (ICA) have used kernel independence measures to obtain very good performance, particularly where classical methods experience difficulty (for instance, sources with near-zero kurtosis). We present Fast Kernel ICA (FastKICA), a novel optimisation technique for one such kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). Our search procedure uses an approximate Newton method on the special orthogonal group, where we estimate the Hessian locally about independence. We employ incomplete Cholesky decomposition to efficiently compute the gradient and approximate Hessian. FastKICA results in more accurate solutions at a given cost compared with gradient descent, and is relatively insensitive to local minima when initialised far from independence. These properties allow kernel approaches to be extended to problems with larger numbers of sources and observations. Our method is competitive with other modern and classical ICA approaches in both speed and accuracy.},
author = {Shen, Hao and Jegelka, Stefanie and Gretton, Arthur},
issn = {15324435},
keywords = {Computational,Information-Theoretic Learning with Statistics,Theory {\&} Algorithms},
pages = {8},
title = {{Fast Kernel ICA using an Approximate Newton Method}},
url = {http://eprints.pascal-network.org/archive/00003141/},
year = {2007}
}
@article{Hyvarinen1999,
abstract = {Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. In this paper, we use a combination of two different approaches for linear ICA: Comon's information-theoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast (objective) functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions. These algorithms optimize the contrast functions very fast and reliably.},
author = {Hyv{\"{a}}rinen, Aapo},
doi = {10.1109/72.761722},
file = {:Users/guillaume/Documents/Mendeley Desktop/Hyv{\"{a}}rinen - 1999 - Survey on independent component analysis.pdf:pdf},
isbn = {0893-6080 (Print)},
issn = {1045-9227},
journal = {Neural Computing Surveys},
number = {3},
pages = {626--34},
pmid = {18252563},
title = {{Survey on independent component analysis}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Survey+on+Independent+Component+Analysis{\#}4{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/18252563},
volume = {10},
year = {1999}
}
@article{Edelman1998,
abstract = {In this paper we develop new Newton and conjugate gradient algorithms on the Grassmann and Stiefel manifolds. These manifolds represent the constraints that arise in such areas as the symmetric eigenvalue problem, nonlinear eigenvalue problems, electronic structures computations, and signal processing. In addition to the new algorithms, we show how the geometrical framework gives penetrating new insights allowing us to create, understand, and compare algorithms. The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical view of previously unrelated algorithms. It is our hope that developers of new algorithms and perturbation theories will benefit from the theory, methods, and examples in this paper.},
archivePrefix = {arXiv},
arxivId = {physics/9806030},
author = {Edelman, Alan and Arias, Tom{\'{a}}s A. and Smith, Steven T.},
doi = {10.1137/S0895479895290954},
eprint = {9806030},
file = {:Users/guillaume/Documents/Mendeley Desktop/Edelman, Arias, Smith - 1998 - The Geometry of Algorithms with Orthogonality Constraints.pdf:pdf},
isbn = {08954798},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {conjugate gradient,eigenvalue optimization,eigenvalues and eigenvectors,electronic,grassmann man-,ifold,invariant subspace,newton,orthogonality constraints,rayleigh quotient iteration,reduced gradient method,s method,sequential quadratic programming,stiefel manifold,structures computation,subspace tracking},
number = {2},
pages = {303--353},
primaryClass = {physics},
title = {{The Geometry of Algorithms with Orthogonality Constraints}},
url = {http://epubs.siam.org/doi/abs/10.1137/S0895479895290954},
volume = {20},
year = {1998}
}
@article{Borgne2004,
author = {Borgne, Herv{\'{e}} Le},
file = {:Users/guillaume/Documents/Mendeley Desktop/Borgne - 2004 - Analyse en Composante Ind{\'{e}}pendantes.pdf:pdf},
pages = {43--182},
title = {{Analyse en Composante Ind{\'{e}}pendantes}},
year = {2004}
}
@article{Hyvarinen2001,
abstract = {A comprehensive introduction to ICA for students and practitioners Independent Component Analysis (ICA) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. This is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. It offers a general overview of the basics of ICA, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more. Independent Component Analysis is divided into four sections that cover: General mathematical concepts utilized in the book The basic ICA model and its solution Various extensions of the basic ICA model Real-world applications for ICA models Authors Hyvarinen, Karhunen, and Oja are well known for their contributions to the development of ICA and here cover all the relevant theory, new algorithms, and applications in various fields. Researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.},
author = {Hyv{\"{a}}rinen, Aapo and Karhunen, Juha and Oja, Erkki},
doi = {10.1016/j.acha.2006.03.003},
file = {:Users/guillaume/Documents/Mendeley Desktop/Hyv{\"{a}}rinen, Karhunen, Oja - 2001 - Independent Component Analysis.pdf:pdf},
isbn = {0471221317},
issn = {10635203},
journal = {Analysis},
number = {1},
pages = {481},
pmid = {20421937},
title = {{Independent Component Analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520306000509},
volume = {26},
year = {2001}
}
@article{Hyvarinen2000,
abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
author = {Hyv{\"{a}}rinen, Aapo and Oja, Erkki},
doi = {10.1016/S0893-6080(00)00026-5},
file = {:Users/guillaume/Documents/Mendeley Desktop/Hyv{\"{a}}rinen, Oja - 2000 - Independent component analysis algorithms and applications.pdf:pdf},
isbn = {3589451327},
issn = {0893-6080},
journal = {Neural networks},
keywords = {Algorithms,Artifacts,Brain,Brain: physiology,Humans,Magnetoencephalography,Neural Networks (Computer),Normal Distribution},
number = {4-5},
pages = {411--430},
pmid = {10946390},
title = {{Independent component analysis: algorithms and applications}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608000000265{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/10946390},
volume = {13},
year = {2000}
}
@article{Winther2007,
abstract = {In this paper we present an empirical Bayesian framework for independent component analysis. The framework provides estimates of the sources, the mixing matrix and the noise parameters, and is flexible with respect to choice of source prior and the number of sources and sensors. Inside the engine of the method are two mean field techniques-the variational Bayes and the expectation consistent framework-and the cost function relating to these methods are optimized using the adaptive overrelaxed expectation maximization (EM) algorithm and the easy gradient recipe. The entire framework, implemented in a Matlab toolbox, is demonstrated for non-negative decompositions and compared with non-negative matrix factorization. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Winther, Ole and Petersen, Kaare Brandt},
doi = {10.1016/j.dsp.2007.01.003},
file = {:Users/guillaume/Documents/Mendeley Desktop/Winther, Petersen - 2007 - Bayesian independent component analysis Variational methods and non-negative decompositions.pdf:pdf},
issn = {10512004},
journal = {Digital Signal Processing: A Review Journal},
number = {5},
pages = {858--872},
title = {{Bayesian independent component analysis: Variational methods and non-negative decompositions}},
volume = {17},
year = {2007}
}
