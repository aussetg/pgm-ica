Automatically generated by Mendeley Desktop 1.17.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{S.AmariA.Cichocki1996,
author = {Amari, S. and Cichocki, A. and Yang, H. H.},
file = {:Users/guillaume/Documents/Mendeley Desktop/Amari, Cichocki, Yang - 1996 - A New Learning Algorithm for Blind Signal Separation.pdf:pdf},
title = {{A New Learning Algorithm for Blind Signal Separation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.7.6283{\&}rep=rep1{\&}type=pdf},
year = {1996}
}
@article{Vaseghi2006,
abstract = {This paper is devoted to practical utilization of Principal Component Analysis (PCA) and its extension Independent Component Analysis (ICA). Our inten-sion is to demonstrate different applications of the above mentioned methods in biomedical image and signal processing. The concept of ICA in terms of blind source separation is illustrated on EEG signals, whereas the approach of sparse coding is explained using fMRI images.},
author = {Vaseghi, S and Jetelov{\'{a}}, H},
file = {:Users/guillaume/Documents/Mendeley Desktop/Vaseghi, Jetelov{\'{a}} - 2006 - Principal and independent component analysis in image processing.pdf:pdf},
journal = {Proceedings of the 14th ACM International Conference on Mobile Computing and Networking (MOBICOM'06)},
number = {2},
pages = {1--5},
title = {{Principal and independent component analysis in image processing}},
year = {2006}
}
@article{Edelman1998,
abstract = {In this paper we develop new Newton and conjugate gradient algorithms on the Grassmann and Stiefel manifolds. These manifolds represent the constraints that arise in such areas as the symmetric eigenvalue problem, nonlinear eigenvalue problems, electronic structures computations, and signal processing. In addition to the new algorithms, we show how the geometrical framework gives penetrating new insights allowing us to create, understand, and compare algorithms. The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical view of previously unrelated algorithms. It is our hope that developers of new algorithms and perturbation theories will benefit from the theory, methods, and examples in this paper.},
archivePrefix = {arXiv},
arxivId = {physics/9806030},
author = {Edelman, Alan and Arias, Tom{\'{a}}s A. and Smith, Steven T.},
doi = {10.1137/S0895479895290954},
eprint = {9806030},
file = {:Users/guillaume/Documents/Mendeley Desktop/Edelman, Arias, Smith - 1998 - The Geometry of Algorithms with Orthogonality Constraints.pdf:pdf},
isbn = {08954798},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {conjugate gradient,eigenvalue optimization,eigenvalues and eigenvectors,electronic,grassmann man-,ifold,invariant subspace,newton,orthogonality constraints,rayleigh quotient iteration,reduced gradient method,s method,sequential quadratic programming,stiefel manifold,structures computation,subspace tracking},
number = {2},
pages = {303--353},
primaryClass = {physics},
title = {{The Geometry of Algorithms with Orthogonality Constraints}},
url = {http://epubs.siam.org/doi/abs/10.1137/S0895479895290954},
volume = {20},
year = {1998}
}
@article{Bell,
abstract = {It has previously been suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, distributed representation of natural scenes, and it has been reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features. We show here that a new unsupervised learning algorithm based on information maximization, a nonlinear “infomax” network, when applied to an ensemble of natural scenes produces sets of visual filters that are localized and oriented. Some of these filters are Gabor-like and resemble those produced by the sparseness-maximization network. In addition, the outputs of these filters are as independent as possible, since this infomax network performs Independent Components Analysis or ICA, for sparse (super-gaussian) component distributions. We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA). The ICA filters have more sparsely distributed (kurtotic) outputs on natural scenes. They also resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form a natural, information-theoretic coordinate system for natural images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bell, Anthony J. and Sejnowski, Terrence J.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/guillaume/Documents/Mendeley Desktop/Bell, Sejnowski - 1997 - Edges are the 'Independent Components' of Natural Scenes.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Vision Research},
number = {23},
pages = {3327--3338},
pmid = {25246403},
title = {{Edges are the 'Independent Components' of Natural Scenes}},
volume = {37},
year = {1997}
}
@article{Shen2007,
abstract = {Recent approaches to independent component analysis (ICA) have used kernel independence measures to obtain very good performance, particularly where classical methods experience difficulty (for instance, sources with near-zero kurtosis). We present Fast Kernel ICA (FastKICA), a novel optimisation technique for one such kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). Our search procedure uses an approximate Newton method on the special orthogonal group, where we estimate the Hessian locally about independence. We employ incomplete Cholesky decomposition to efficiently compute the gradient and approximate Hessian. FastKICA results in more accurate solutions at a given cost compared with gradient descent, and is relatively insensitive to local minima when initialised far from independence. These properties allow kernel approaches to be extended to problems with larger numbers of sources and observations. Our method is competitive with other modern and classical ICA approaches in both speed and accuracy.},
author = {Shen, Hao and Jegelka, Stefanie and Gretton, Arthur},
file = {:Users/guillaume/Documents/Mendeley Desktop/Shen, Jegelka, Gretton - 2007 - Fast Kernel ICA using an Approximate Newton Method.pdf:pdf},
issn = {15324435},
keywords = {Computational,Information-Theoretic Learning with Statistics,Theory {\&} Algorithms},
pages = {8},
title = {{Fast Kernel ICA using an Approximate Newton Method}},
url = {http://eprints.pascal-network.org/archive/00003141/},
year = {2007}
}
@article{Hyvarinen1999,
abstract = {Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. In this paper, we use a combination of two different approaches for linear ICA: Comon's information-theoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast (objective) functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions. These algorithms optimize the contrast functions very fast and reliably.},
author = {Hyv{\"{a}}rinen, Aapo},
doi = {10.1109/72.761722},
file = {:Users/guillaume/Documents/Mendeley Desktop/Hyv{\"{a}}rinen - 1999 - Survey on independent component analysis.pdf:pdf},
isbn = {0893-6080 (Print)},
issn = {1045-9227},
journal = {Neural Computing Surveys},
number = {3},
pages = {626--34},
pmid = {18252563},
title = {{Survey on independent component analysis}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Survey+on+Independent+Component+Analysis{\#}4{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/18252563},
volume = {10},
year = {1999}
}
@article{Winther2007,
abstract = {In this paper we present an empirical Bayesian framework for independent component analysis. The framework provides estimates of the sources, the mixing matrix and the noise parameters, and is flexible with respect to choice of source prior and the number of sources and sensors. Inside the engine of the method are two mean field techniques-the variational Bayes and the expectation consistent framework-and the cost function relating to these methods are optimized using the adaptive overrelaxed expectation maximization (EM) algorithm and the easy gradient recipe. The entire framework, implemented in a Matlab toolbox, is demonstrated for non-negative decompositions and compared with non-negative matrix factorization. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Winther, Ole and Petersen, Kaare Brandt},
doi = {10.1016/j.dsp.2007.01.003},
file = {:Users/guillaume/Documents/Mendeley Desktop/Winther, Petersen - 2007 - Bayesian independent component analysis Variational methods and non-negative decompositions.pdf:pdf},
issn = {10512004},
journal = {Digital Signal Processing: A Review Journal},
number = {5},
pages = {858--872},
title = {{Bayesian independent component analysis: Variational methods and non-negative decompositions}},
volume = {17},
year = {2007}
}
@article{Cardoso2003,
abstract = {Abstract Independent component analysis (ICA) is the decomposition of a random vector in linear components which are" as independent as possible." Here," independence" should be understood in its strong statistical sense: it goes beyond (second-order) decorrelation ... $\backslash$n},
author = {Cardoso, Jean-Fran{\c{c}}ois},
doi = {10.1162/jmlr.2003.4.7-8.1177},
file = {:Users/guillaume/Documents/Mendeley Desktop/Cardoso - 2003 - Dependence, correlation and Gaussianity in independent component analysis.pdf:pdf},
isbn = {1532-4435},
issn = {0003-6951},
journal = {The Journal of Machine Learning Research},
keywords = {cumulant expansions,independent component analysis,information,information geometry,minimum entropy,mutual,non-gaussianity,source separation},
pages = {1177--1203},
pmid = {501},
title = {{Dependence, correlation and Gaussianity in independent component analysis}},
url = {http://dl.acm.org/citation.cfm?id=945365.964302{\%}5Cnpapers2://publication/uuid/D8E63C91-C250-49FC-92A0-E13348D8F125},
volume = {4},
year = {2003}
}
@article{Hyvarinen2000,
abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
author = {Hyv{\"{a}}rinen, Aapo and Oja, Erkki},
doi = {10.1016/S0893-6080(00)00026-5},
file = {:Users/guillaume/Documents/Mendeley Desktop/Hyv{\"{a}}rinen, Oja - 2000 - Independent component analysis algorithms and applications.pdf:pdf},
isbn = {3589451327},
issn = {0893-6080},
journal = {Neural networks},
keywords = {Algorithms,Artifacts,Brain,Brain: physiology,Humans,Magnetoencephalography,Neural Networks (Computer),Normal Distribution},
number = {4-5},
pages = {411--430},
pmid = {10946390},
title = {{Independent component analysis: algorithms and applications}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608000000265{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/10946390},
volume = {13},
year = {2000}
}
@phdthesis{Choudrey2002,
author = {Choudrey, Rizwan A},
file = {:Users/guillaume/Documents/Mendeley Desktop/Choudrey - 2002 - Variational Methods for Bayesian Independent Component Analysis.pdf:pdf},
title = {{Variational Methods for Bayesian Independent Component Analysis}},
year = {2002}
}
@article{Hyvarinen1998,
author = {Hyv{\"{a}}rinen, Aapo},
file = {:Users/guillaume/Documents/Mendeley Desktop/Hyv{\"{a}}rinen - 1998 - Independent component analysis in the presence of gaussian noise by maximising joint likelihood.pdf:pdf},
journal = {Neurocomputing},
keywords = {aapo,blind source separation,competitive learning,fi,hut,hyvarinen,independent component analysis,likelihood,maximum,neural networks},
pages = {1--},
title = {{Independent component analysis in the presence of gaussian noise by maximising joint likelihood}},
year = {1998}
}
@article{Shen2009,
author = {Shen, Hao and Jegelka, Stefanie and Gretton, Arthur},
file = {:Users/guillaume/Documents/Mendeley Desktop/Shen, Jegelka, Gretton - 2009 - Fast Kernel-Based Independent Component Analysis.pdf:pdf},
journal = {IEEE Transactions on Signal Processing},
title = {{Fast Kernel-Based Independent Component Analysis}},
volume = {57},
year = {2009}
}
@article{Gretton2005,
abstract = {We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is verified in the context of independent component analysis.},
author = {Gretton, A. and Herbrich, R. and Smola, A. J. and Bousquet, O. and Sch{\"{o}}lkopf, B.},
file = {:Users/guillaume/Documents/Mendeley Desktop/Gretton et al. - 2005 - Kernel Methods for Measuring Independence.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {2075--2129},
title = {{Kernel Methods for Measuring Independence}},
url = {http://eprints.pascal-network.org/archive/00001702/},
volume = {6},
year = {2005}
}
@article{Hyvarinen,
author = {Hyvarinen, Aapo},
file = {:Users/guillaume/Documents/Mendeley Desktop/Hyvarinen - Unknown - Indipendent Component Analysis for Time-dependent Stochastic Processes.pdf:pdf},
pages = {5--10},
title = {{Indipendent Component Analysis for Time-dependent Stochastic Processes}}
}
@article{Borgne2004,
author = {Borgne, Herv{\'{e}} Le},
file = {:Users/guillaume/Documents/Mendeley Desktop/Borgne - 2004 - Analyse en Composante Ind{\'{e}}pendantes.pdf:pdf},
pages = {43--182},
title = {{Analyse en Composante Ind{\'{e}}pendantes}},
year = {2004}
}
@article{Lawrence2000,
author = {Lawrence, Neil D and Bishop, Christopher M},
file = {:Users/guillaume/Documents/Mendeley Desktop/Lawrence, Bishop - 2000 - Variational Bayesian independent component analysis.pdf:pdf},
journal = {Univ of Cambridge Tech Report},
number = {2},
title = {{Variational Bayesian independent component analysis}},
year = {2000}
}
@article{Gretton2005a,
abstract = {We propose an independence criterion based on the eigen- $\backslash$r$\backslash$nspectrum of covariance operators in reproducing kernel Hilbert spaces $\backslash$r$\backslash$n(RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt $\backslash$r$\backslash$nnorm of the cross-covariance operator (we term this a Hilbert-Schmidt In- $\backslash$r$\backslash$ndependence Criterion, or HSIC). This approach has several advantages, $\backslash$r$\backslash$ncompared with previous kernel-based independence criteria. First, the $\backslash$r$\backslash$nempirical estimate is simpler than any other kernel dependence test, and $\backslash$r$\backslash$nrequires no user-defined regularisation. Second, there is a clearly defined $\backslash$r$\backslash$npopulation quantity which the empirical estimate approaches in the large $\backslash$r$\backslash$nsample limit, with exponential convergence guaranteed between the two: $\backslash$r$\backslash$nthis ensures that independence tests based on HSIC do not suffer from $\backslash$r$\backslash$nslow learning rates. Finally, we show in the context of independent com- $\backslash$r$\backslash$nponent analysis (ICA) that the performance of HSIC is competitive with $\backslash$r$\backslash$nthat of previously published kernel-based criteria, and of other recently $\backslash$r$\backslash$npublished ICA methods.},
author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Scḧlkopf, Bernhard},
doi = {10.1007/11564089_7},
file = {:Users/guillaume/Documents/Mendeley Desktop/Gretton et al. - 2005 - Measuring statistical dependence with Hilbert-Schmidt norms.pdf:pdf},
isbn = {354029242X},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {63--77},
title = {{Measuring statistical dependence with Hilbert-Schmidt norms}},
volume = {3734 LNAI},
year = {2005}
}
@article{Bach2002,
author = {Bach, Francis R and Jordan, Michael I},
doi = {10.1162/153244303768966085},
file = {:Users/guillaume/Documents/Mendeley Desktop/Bach, Jordan - 2002 - Kernel Independent Component Analysis.pdf:pdf},
isbn = {0780376633},
issn = {0003-6951},
journal = {Journal of Machine Learning Research},
keywords = {blind source separation,canonical correlations,gral equations,gram matrices,incomplete cholesky decomposition,independent component analysis,inte-,kernel methods,mutual information,semiparametric models,stiefel manifold},
pages = {1--48},
pmid = {181462700001},
title = {{Kernel Independent Component Analysis}},
volume = {3},
year = {2002}
}
@article{Hyvarinen2001,
abstract = {A comprehensive introduction to ICA for students and practitioners Independent Component Analysis (ICA) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. This is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. It offers a general overview of the basics of ICA, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more. Independent Component Analysis is divided into four sections that cover: General mathematical concepts utilized in the book The basic ICA model and its solution Various extensions of the basic ICA model Real-world applications for ICA models Authors Hyvarinen, Karhunen, and Oja are well known for their contributions to the development of ICA and here cover all the relevant theory, new algorithms, and applications in various fields. Researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.},
author = {Hyv{\"{a}}rinen, Aapo and Karhunen, Juha and Oja, Erkki},
doi = {10.1016/j.acha.2006.03.003},
file = {:Users/guillaume/Documents/Mendeley Desktop/Hyv{\"{a}}rinen, Karhunen, Oja - 2001 - Independent Component Analysis.pdf:pdf},
isbn = {0471221317},
issn = {10635203},
journal = {Analysis},
number = {1},
pages = {481},
pmid = {20421937},
title = {{Independent Component Analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520306000509},
volume = {26},
year = {2001}
}
